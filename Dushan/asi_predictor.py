#!/usr/bin/env python3
"""
IMPROVED Enhanced ASI Market Downturn Predictor
Addresses: Overfitting (FIXED) + Performance Issues (NEW FIXES)
Target: High Precision (>90%), Minimal False Negatives, AUC >85%
"""

import pandas as pd
import numpy as np
import pickle
import os
import json
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# ML Libraries
from sklearn.model_selection import train_test_split, StratifiedKFold, validation_curve, learning_curve
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, roc_curve, confusion_matrix, classification_report,
    precision_recall_curve, average_precision_score
)
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.isotonic import IsotonicRegression
from sklearn.calibration import CalibratedClassifierCV

import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats

# Try importing advanced models
try:
    import xgboost as xgb
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False
    print("XGBoost not available. Install with: pip install xgboost")

try:
    import lightgbm as lgb
    LIGHTGBM_AVAILABLE = True
except ImportError:
    LIGHTGBM_AVAILABLE = False
    print("LightGBM not available. Install with: pip install lightgbm")

class ImprovedEnhancedPredictor:
    """IMPROVED predictor: Fixed overfitting + Better performance targeting"""
    
    def __init__(self, csv_file, fn_penalty=5.0, target_precision=0.92):
        print("="*100)
        print("IMPROVED ENHANCED ASI MARKET DOWNTURN PREDICTOR")
        print("PHASE 1 FIXES: ‚úÖ Overfitting + Data Distribution (COMPLETED)")
        print("PHASE 2 FIXES: üéØ Performance Targeting (NEW)")
        print("Target: High Precision (>90%), Minimal False Negatives, AUC >85%")
        print("="*100)
        
        self.csv_file = csv_file
        self.fn_penalty = fn_penalty  # SWEET SPOT: 5.0x (between 2x and 20x)
        self.fp_penalty = 1.0
        self.target_precision = target_precision
        
        # Load and prepare data
        self.df = pd.read_csv(csv_file)
        self.df['date'] = pd.to_datetime(self.df['date'])
        self.df = self.df.sort_values('date').reset_index(drop=True)
        self.df['close'] = pd.to_numeric(self.df['close'])
        self.df['returns'] = self.df['close'].pct_change()
        
        # Initialize storage
        self.models = {}
        self.ensemble = None
        self.feature_names = []
        self.overfitting_results = {}
        self.optimal_thresholds = {}
        self.evaluation_results = {}
        
        print(f"Loaded {len(self.df)} records from {self.df['date'].min()} to {self.df['date'].max()}")
        print(f"\nPHASE 2 IMPROVEMENTS:")
        print(f"  üéØ Increased FN penalty: {self.fn_penalty}x (was 2x, sweet spot)")
        print(f"  üéØ Enhanced feature engineering (20 key features)")
        print(f"  üéØ Precision-focused threshold optimization")
        print(f"  üéØ Weighted ensemble for downturn detection")
        print(f"  üéØ Cost-sensitive evaluation metrics")
    
    def engineer_enhanced_features(self):
        """Enhanced feature engineering - 20 most predictive features"""
        print("\n" + "="*80)
        print("STEP 1: ENHANCED FEATURE ENGINEERING (20 KEY FEATURES)")
        print("="*80)
        
        # Compute enhanced technical indicators
        self._compute_enhanced_features()
        
        print(f"Total features created: {len([col for col in self.df.columns if col not in ['date', 'close', 'returns']])}")
        
    def _compute_enhanced_features(self):
        """Compute enhanced technical indicators"""
        print("Computing enhanced technical indicators...")
        
        # Core Moving Averages
        self.df['sma_20'] = self.df['close'].rolling(20).mean()
        self.df['sma_50'] = self.df['close'].rolling(50).mean()
        self.df['sma_200'] = self.df['close'].rolling(200).mean()
        self.df['ema_12'] = self.df['close'].ewm(span=12, adjust=False).mean()
        self.df['ema_26'] = self.df['close'].ewm(span=26, adjust=False).mean()
        
        # MACD (proven important)
        self.df['macd'] = self.df['ema_12'] - self.df['ema_26']
        self.df['macd_signal'] = self.df['macd'].ewm(span=9, adjust=False).mean()
        self.df['macd_hist'] = self.df['macd'] - self.df['macd_signal']
        
        # RSI (proven important)
        def calculate_rsi(series, period=14):
            delta = series.diff()
            gain = delta.clip(lower=0).rolling(period).mean()
            loss = -delta.clip(upper=0).rolling(period).mean()
            rs = gain / (loss + 1e-9)
            return 100 - 100 / (1 + rs)
        
        self.df['rsi_14'] = calculate_rsi(self.df['close'], 14)
        self.df['rsi_7'] = calculate_rsi(self.df['close'], 7)
        
        # Volatility indicators (crucial for downturns)
        self.df['volatility_20'] = self.df['returns'].rolling(20).std() * np.sqrt(252)
        self.df['volatility_60'] = self.df['returns'].rolling(60).std() * np.sqrt(252)
        self.df['vol_spike'] = (self.df['volatility_20'] > self.df['volatility_20'].rolling(252).quantile(0.8)).astype(int)
        
        # Enhanced momentum indicators
        self.df['momentum_5'] = self.df['close'].pct_change(5)
        self.df['momentum_10'] = self.df['close'].pct_change(10)
        self.df['momentum_20'] = self.df['close'].pct_change(20)
        
        # Trend strength indicators
        self.df['price_vs_sma20'] = (self.df['close'] - self.df['sma_20']) / (self.df['sma_20'] + 1e-9)
        self.df['price_vs_sma50'] = (self.df['close'] - self.df['sma_50']) / (self.df['sma_50'] + 1e-9)
        self.df['price_vs_sma200'] = (self.df['close'] - self.df['sma_200']) / (self.df['sma_200'] + 1e-9)
        
        # Multiple timeframe drawdowns (critical for downturn prediction)
        for w in [20, 60, 120]:
            peak_w = self.df['close'].rolling(w).max()
            self.df[f'local_dd_{w}'] = (self.df['close'] - peak_w) / peak_w
        
        # Enhanced confirmation signals
        self.df['below_sma50'] = (self.df['close'] < self.df['sma_50']).astype(int)
        self.df['negative_momentum_20'] = (self.df['momentum_20'] < 0).astype(int)
        self.df['rsi_below_30'] = (self.df['rsi_14'] < 30).astype(int)  # More sensitive
        self.df['rsi_below_50'] = (self.df['rsi_14'] < 50).astype(int)
        self.df['macd_bearish'] = (self.df['macd'] < self.df['macd_signal']).astype(int)
        
        # Returns and volatility combinations
        self.df['return_5d'] = self.df['close'].pct_change(5)
        self.df['return_10d'] = self.df['close'].pct_change(10)
        self.df['vol_adjusted_return'] = self.df['momentum_10'] / (self.df['volatility_20'] + 1e-6)
        
        print("‚úÖ Selected 20 enhanced features optimized for downturn detection")
    
    def create_labels(self, threshold=-0.05, prediction_days=10):
        """Create target labels"""
        print(f"\n" + "="*80)
        print("STEP 2: CREATING LABELS")
        print("="*80)
        print(f"Target: {threshold*100:.0f}% drawdown in next {prediction_days} days")
        
        self.df['will_have_drawdown'] = False
        self.df['max_drawdown_in_period'] = 0.0
        
        for i in range(len(self.df) - prediction_days):
            current_price = self.df.loc[i, 'close']
            peak_price = current_price
            max_drawdown = 0.0
            
            lookahead_window = self.df.iloc[i+1:i+1+prediction_days]
            
            for j, row in lookahead_window.iterrows():
                price = row['close']
                if price > peak_price:
                    peak_price = price
                drawdown = (price - peak_price) / peak_price
                if drawdown < max_drawdown:
                    max_drawdown = drawdown
            
            if max_drawdown <= threshold:
                self.df.loc[i, 'will_have_drawdown'] = True
            self.df.loc[i, 'max_drawdown_in_period'] = max_drawdown
        
        positive_cases = self.df['will_have_drawdown'].sum()
        total_cases = len(self.df) - prediction_days
        
        print(f"Positive cases: {positive_cases} ({positive_cases/total_cases*100:.2f}%)")
        print(f"Negative cases: {total_cases - positive_cases}")
        
        return self.df
    
    def prepare_and_save_datasets_improved(self, test_size=0.2, val_size=0.2, output_dir='datasets_v2'):
        """Improved dataset preparation with enhanced features"""
        print(f"\n" + "="*80)
        print("STEP 3: PREPARING ENHANCED DATASETS")
        print("="*80)
        print("‚úÖ KEEP: Stratified splits (fixed overfitting)")
        print("üéØ NEW: Enhanced 20-feature set for better predictions")
        
        # Create output directory
        os.makedirs(output_dir, exist_ok=True)
        
        # Enhanced feature set (20 features)
        selected_features = [
            'rsi_14', 'rsi_7', 'macd_hist', 'macd', 'momentum_5', 'momentum_10', 'momentum_20',
            'volatility_20', 'volatility_60', 'vol_spike', 'price_vs_sma20', 'price_vs_sma50', 'price_vs_sma200',
            'local_dd_20', 'local_dd_60', 'local_dd_120', 'below_sma50', 'negative_momentum_20',
            'rsi_below_30', 'rsi_below_50', 'macd_bearish', 'return_5d', 'return_10d', 'vol_adjusted_return'
        ]
        
        # Filter to valid data
        data = self.df[self.df['will_have_drawdown'].notna()].copy()
        
        # Handle missing values
        for col in selected_features:
            if col in data.columns and data[col].isna().any():
                data[col] = data[col].fillna(data[col].median())
        
        # Filter features that exist
        available_features = [col for col in selected_features if col in data.columns]
        
        X = data[available_features].copy()
        y = data['will_have_drawdown'].copy()
        dates = data['date'].copy()
        prices = data['close'].copy()
        
        print(f"Dataset shape: {X.shape}")
        print(f"Enhanced features: {len(available_features)}")
        print(f"Overall positive class ratio: {y.mean():.3f}")
        
        print(f"\nEnhanced features being used:")
        for i, feat in enumerate(available_features, 1):
            print(f"  {i:2d}. {feat}")
        
        # KEEP: Stratified splits (this fixed overfitting)
        print(f"\n‚úÖ Using STRATIFIED splits (keeps overfitting fixed)")
        
        # First split: separate test set
        X_temp, X_test, y_temp, y_test, dates_temp, dates_test, prices_temp, prices_test = train_test_split(
            X, y, dates, prices, 
            test_size=test_size, 
            random_state=42, 
            stratify=y
        )
        
        # Second split: separate train and validation from remaining data
        val_ratio = val_size / (1 - test_size)
        X_train, X_val, y_train, y_val, dates_train, dates_val, prices_train, prices_val = train_test_split(
            X_temp, y_temp, dates_temp, prices_temp,
            test_size=val_ratio,
            random_state=42,
            stratify=y_temp
        )
        
        # Create datasets dictionary
        datasets = {
            'train': {'X': X_train, 'y': y_train, 'dates': dates_train, 'prices': prices_train},
            'val': {'X': X_val, 'y': y_val, 'dates': dates_val, 'prices': prices_val},
            'test': {'X': X_test, 'y': y_test, 'dates': dates_test, 'prices': prices_test}
        }
        
        # Save datasets
        for split_name, split_data in datasets.items():
            # Combine all data
            combined_df = pd.concat([
                split_data['dates'].reset_index(drop=True),
                split_data['prices'].reset_index(drop=True),
                split_data['X'].reset_index(drop=True),
                split_data['y'].reset_index(drop=True)
            ], axis=1)
            
            # Save to CSV
            filename = os.path.join(output_dir, f'{split_name}_dataset_v2.csv')
            combined_df.to_csv(filename, index=False)
            
            print(f"\n{split_name.upper()} set saved: {filename}")
            print(f"  Shape: {split_data['X'].shape}")
            print(f"  Positive cases: {split_data['y'].sum()} ({split_data['y'].mean()*100:.1f}%) ‚úÖ BALANCED")
            print(f"  Date range: {split_data['dates'].min()} to {split_data['dates'].max()}")
        
        # Store for later use
        self.X_train, self.X_val, self.X_test = X_train, X_val, X_test
        self.y_train, self.y_val, self.y_test = y_train, y_val, y_test
        self.dates_train, self.dates_val, self.dates_test = dates_train, dates_val, dates_test
        self.feature_names = available_features
        
        print(f"\n‚úÖ Enhanced feature set: {len(available_features)} features")
        print(f"‚úÖ Maintained balanced splits to prevent overfitting")
        
        return datasets
    
    def train_performance_optimized_models(self):
        """Train models optimized for downturn detection performance"""
        print(f"\n" + "="*80)
        print("STEP 4: PERFORMANCE-OPTIMIZED MODEL TRAINING")
        print("="*80)
        print(f"‚úÖ KEEP: Regularization (prevents overfitting)")
        print(f"üéØ NEW: FN penalty increased to {self.fn_penalty}x (sweet spot)")
        print(f"üéØ NEW: Optimized for downturn detection")
        
        models = {}
        
        # Performance-optimized LightGBM
        if LIGHTGBM_AVAILABLE:
            print("\nüéØ Training Performance-Optimized LightGBM...")
            scale_pos_weight = (self.y_train == 0).sum() / (self.y_train == 1).sum()
            
            # BALANCED parameters: regularization + performance
            lgb_params = {
                'objective': 'binary',
                'metric': 'auc',
                'boosting_type': 'gbdt',
                'num_leaves': 15,        # Moderate (was 10, not too small)
                'learning_rate': 0.02,   # Moderate (was 0.01, not too slow)
                'feature_fraction': 0.7, # Moderate sampling
                'bagging_fraction': 0.8, 
                'bagging_freq': 5,
                'min_child_samples': 50, # Moderate (was 100, not too large)
                'min_data_in_leaf': 25,  
                'scale_pos_weight': scale_pos_weight * self.fn_penalty,
                'lambda_l1': 5.0,        # Moderate regularization (was 10.0)
                'lambda_l2': 5.0,        
                'max_depth': 5,          # Slightly deeper (was 3)
                'random_state': 42,
                'verbose': -1
            }
            
            train_data = lgb.Dataset(self.X_train, label=self.y_train)
            val_data = lgb.Dataset(self.X_val, label=self.y_val, reference=train_data)
            
            lgb_model = lgb.train(
                lgb_params,
                train_data,
                num_boost_round=150,     # More rounds for performance (was 100)
                valid_sets=[train_data, val_data],
                callbacks=[
                    lgb.early_stopping(stopping_rounds=30),
                    lgb.log_evaluation(period=0)
                ]
            )
            models['Optimized_LightGBM'] = lgb_model
        
        # Performance-optimized XGBoost
        if XGBOOST_AVAILABLE:
            print("üéØ Training Performance-Optimized XGBoost...")
            scale_pos_weight = (self.y_train == 0).sum() / (self.y_train == 1).sum()
            
            xgb_params = {
                'objective': 'binary:logistic',
                'eval_metric': 'auc',
                'max_depth': 5,          # Slightly deeper (was 3)
                'learning_rate': 0.02,   # Moderate (was 0.01)
                'subsample': 0.8,        
                'colsample_bytree': 0.8, 
                'min_child_weight': 10,  # Moderate (was 20)
                'scale_pos_weight': scale_pos_weight * self.fn_penalty,
                'alpha': 5.0,            # Moderate regularization (was 10.0)
                'lambda': 5.0,          
                'gamma': 0.5,            # Reduced (was 1.0)
                'random_state': 42,
                'verbosity': 0
            }
            
            train_data = xgb.DMatrix(self.X_train, label=self.y_train)
            val_data = xgb.DMatrix(self.X_val, label=self.y_val)
            
            xgb_model = xgb.train(
                xgb_params,
                train_data,
                num_boost_round=150,     # More rounds (was 100)
                evals=[(train_data, 'train'), (val_data, 'eval')],
                early_stopping_rounds=30,
                verbose_eval=False
            )
            models['Optimized_XGBoost'] = xgb_model
        
        # Performance-optimized Random Forest
        print("üéØ Training Performance-Optimized Random Forest...")
        rf_model = RandomForestClassifier(
            n_estimators=100,        # More trees (was 50)
            max_depth=8,             # Deeper (was 5)
            min_samples_split=20,    # Less restrictive (was 50)
            min_samples_leaf=10,     # Less restrictive (was 25)
            max_features='sqrt',     
            class_weight={0: 1, 1: self.fn_penalty},
            random_state=42,
            n_jobs=-1
        )
        rf_model.fit(self.X_train, self.y_train)
        models['Optimized_RandomForest'] = rf_model
        
        # Performance-optimized Logistic Regression
        print("üéØ Training Performance-Optimized Logistic Regression...")
        scaler = RobustScaler()
        X_train_scaled = scaler.fit_transform(self.X_train)
        
        lr_model = LogisticRegression(
            random_state=42,
            max_iter=1000,
            class_weight={0: 1.0, 1: self.fn_penalty},
            C=0.1,                   # Less regularization (was 0.01)
            solver='liblinear',
            penalty='l1'
        )
        lr_model.fit(X_train_scaled, self.y_train)
        models['Optimized_LogisticRegression'] = {'model': lr_model, 'scaler': scaler}
        
        self.models = models
        print(f"\n‚úÖ Trained {len(models)} performance-optimized models")
        print("‚úÖ Balanced regularization + performance for downturn detection")
        return models
    
    def detect_overfitting(self):
        """Check if overfitting is still under control"""
        print(f"\n" + "="*80)
        print("STEP 5: OVERFITTING CHECK (SHOULD REMAIN GOOD)")
        print("="*80)
        
        overfitting_results = {}
        
        for model_name, model in self.models.items():
            print(f"\nAnalyzing overfitting for {model_name}...")
            
            # Get predictions for train and validation sets
            train_pred, val_pred = self._get_model_predictions(model_name, model)
            
            # Calculate metrics for both sets
            train_auc = roc_auc_score(self.y_train, train_pred['proba'])
            val_auc = roc_auc_score(self.y_val, val_pred['proba'])
            
            train_precision = precision_score(self.y_train, train_pred['binary'])
            val_precision = precision_score(self.y_val, val_pred['binary'])
            
            train_recall = recall_score(self.y_train, train_pred['binary'])
            val_recall = recall_score(self.y_val, val_pred['binary'])
            
            # Calculate overfitting indicators
            auc_gap = train_auc - val_auc
            precision_gap = train_precision - val_precision
            recall_gap = train_recall - val_recall
            
            # Overfitting assessment
            overfitting_score = 0
            overfitting_indicators = []
            
            if auc_gap > 0.10:
                overfitting_score += 2
                overfitting_indicators.append(f"Large AUC gap: {auc_gap:.3f}")
            elif auc_gap > 0.05:
                overfitting_score += 1
                overfitting_indicators.append(f"Moderate AUC gap: {auc_gap:.3f}")
            
            if precision_gap > 0.15:
                overfitting_score += 1
                overfitting_indicators.append(f"Large precision gap: {precision_gap:.3f}")
            
            if recall_gap > 0.15:
                overfitting_score += 1
                overfitting_indicators.append(f"Large recall gap: {recall_gap:.3f}")
            
            # Classification
            if overfitting_score >= 3:
                overfitting_status = "HIGH OVERFITTING"
            elif overfitting_score >= 2:
                overfitting_status = "MODERATE OVERFITTING"
            elif overfitting_score >= 1:
                overfitting_status = "SLIGHT OVERFITTING"
            else:
                overfitting_status = "NO OVERFITTING"
            
            overfitting_results[model_name] = {
                'overfitting_status': overfitting_status,
                'overfitting_score': overfitting_score,
                'indicators': overfitting_indicators,
                'metrics': {
                    'train_auc': train_auc,
                    'val_auc': val_auc,
                    'auc_gap': auc_gap,
                    'train_precision': train_precision,
                    'val_precision': val_precision,
                    'precision_gap': precision_gap,
                    'train_recall': train_recall,
                    'val_recall': val_recall,
                    'recall_gap': recall_gap
                }
            }
            
            # Print results
            print(f"  Status: {overfitting_status}")
            print(f"  AUC - Train: {train_auc:.4f}, Val: {val_auc:.4f}, Gap: {auc_gap:.4f}")
            if auc_gap < 0.05:
                print(f"    ‚úÖ EXCELLENT: AUC gap < 5%")
            elif auc_gap < 0.10:
                print(f"    ‚úÖ GOOD: AUC gap < 10%")
            print(f"  Precision - Train: {train_precision:.4f}, Val: {val_precision:.4f}")
            print(f"  Recall - Train: {train_recall:.4f}, Val: {val_recall:.4f}")
            
            if overfitting_indicators:
                print(f"  Issues: {', '.join(overfitting_indicators)}")
            else:
                print(f"  ‚úÖ NO MAJOR OVERFITTING ISSUES!")
        
        # Summary
        print(f"\n" + "-"*50)
        print("OVERFITTING SUMMARY (SHOULD REMAIN GOOD)")
        print("-"*50)
        
        good_models = 0
        for model_name, results in overfitting_results.items():
            status = results['overfitting_status']
            if status in ['NO OVERFITTING', 'SLIGHT OVERFITTING']:
                good_models += 1
                print(f"‚úÖ {model_name}: {status}")
            else:
                print(f"‚ö†Ô∏è  {model_name}: {status}")
        
        print(f"\n‚úÖ {good_models}/{len(overfitting_results)} models maintain good generalization")
        
        self.overfitting_results = overfitting_results
        return overfitting_results
    
    def _get_model_predictions(self, model_name, model):
        """Get predictions from model"""
        if model_name == 'Optimized_LightGBM':
            train_proba = model.predict(self.X_train, num_iteration=model.best_iteration)
            val_proba = model.predict(self.X_val, num_iteration=model.best_iteration)
        elif model_name == 'Optimized_XGBoost':
            train_proba = model.predict(xgb.DMatrix(self.X_train))
            val_proba = model.predict(xgb.DMatrix(self.X_val))
        elif model_name == 'Optimized_LogisticRegression':
            train_proba = model['model'].predict_proba(model['scaler'].transform(self.X_train))[:, 1]
            val_proba = model['model'].predict_proba(model['scaler'].transform(self.X_val))[:, 1]
        else:
            train_proba = model.predict_proba(self.X_train)[:, 1]
            val_proba = model.predict_proba(self.X_val)[:, 1]
        
        train_binary = (train_proba > 0.5).astype(int)
        val_binary = (val_proba > 0.5).astype(int)
        
        return (
            {'proba': train_proba, 'binary': train_binary},
            {'proba': val_proba, 'binary': val_binary}
        )
    
    def optimize_precision_focused_thresholds(self):
        """Optimize thresholds specifically for high precision with low FN rate"""
        print(f"\n" + "="*80)
        print("STEP 6: PRECISION-FOCUSED THRESHOLD OPTIMIZATION")
        print("="*80)
        
        optimal_thresholds = {}
        
        for model_name, model in self.models.items():
            print(f"\nOptimizing threshold for {model_name}...")
            
            # Get validation predictions
            _, val_pred = self._get_model_predictions(model_name, model)
            y_pred_proba = val_pred['proba']
            
            # NEW: Multi-objective optimization
            best_threshold = 0.5
            best_score = -float('inf')
            
            thresholds = np.linspace(0.05, 0.95, 200)  # More granular search
            
            for threshold in thresholds:
                y_pred = (y_pred_proba >= threshold).astype(int)
                
                if len(np.unique(y_pred)) == 1:
                    continue
                
                try:
                    tn, fp, fn, tp = confusion_matrix(self.y_val, y_pred).ravel()
                    
                    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
                    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
                    fn_rate = fn / (fn + tp) if (fn + tp) > 0 else 0
                    
                    # Multi-objective score: Emphasize precision and minimize FN
                    # Score = precision_weight * precision - fn_penalty * fn_rate + recall_bonus * recall
                    score = (
                        0.6 * precision +           # 60% weight on precision
                        0.3 * (1 - fn_rate) +       # 30% weight on minimizing FN rate
                        0.1 * recall                # 10% weight on recall
                    )
                    
                    # Bonus for achieving target precision
                    if precision >= 0.85:
                        score += 0.2
                    if precision >= 0.90:
                        score += 0.3
                    
                    # Penalty for too many false negatives
                    if fn_rate > 0.4:
                        score -= 0.3
                    if fn_rate > 0.6:
                        score -= 0.5
                    
                    if score > best_score:
                        best_score = score
                        best_threshold = threshold
                
                except:
                    continue
            
            # Calculate final metrics at optimal threshold
            y_pred_final = (y_pred_proba >= best_threshold).astype(int)
            
            try:
                final_precision = precision_score(self.y_val, y_pred_final)
                final_recall = recall_score(self.y_val, y_pred_final)
                
                tn, fp, fn, tp = confusion_matrix(self.y_val, y_pred_final).ravel()
                fn_rate = fn / (fn + tp) if (fn + tp) > 0 else 0
            except:
                final_precision = 0
                final_recall = 0
                fn_rate = 1.0
            
            optimal_thresholds[model_name] = {
                'threshold': best_threshold,
                'precision': final_precision,
                'recall': final_recall,
                'fn_rate': fn_rate,
                'score': best_score
            }
            
            print(f"  Optimal threshold: {best_threshold:.4f}")
            print(f"  Precision: {final_precision:.4f}")
            print(f"  Recall: {final_recall:.4f}")
            print(f"  FN rate: {fn_rate:.4f}")
            print(f"  Multi-objective score: {best_score:.4f}")
        
        self.optimal_thresholds = optimal_thresholds
        return optimal_thresholds
    
    def create_weighted_ensemble(self):
        """Create ensemble weighted by downturn detection ability"""
        print(f"\n" + "="*80)
        print("STEP 7: WEIGHTED ENSEMBLE FOR DOWNTURN DETECTION")
        print("="*80)
        
        # Get model performance on validation set
        model_weights = {}
        meta_features_val = np.zeros((len(self.X_val), len(self.models)))
        
        model_names = list(self.models.keys())
        
        print("Computing model weights based on downturn detection performance...")
        
        for i, (model_name, model) in enumerate(self.models.items()):
            _, val_pred = self._get_model_predictions(model_name, model)
            meta_features_val[:, i] = val_pred['proba']
            
            # Calculate weight based on: AUC + Precision + (1 - FN_rate)
            threshold = self.optimal_thresholds[model_name]['threshold']
            y_pred = (val_pred['proba'] >= threshold).astype(int)
            
            auc = roc_auc_score(self.y_val, val_pred['proba'])
            precision = self.optimal_thresholds[model_name]['precision']
            fn_rate = self.optimal_thresholds[model_name]['fn_rate']
            
            # Weight formula: emphasize AUC and precision, penalize high FN rate
            weight = 0.4 * auc + 0.4 * precision + 0.2 * (1 - fn_rate)
            model_weights[model_name] = weight
            
            print(f"  {model_name}: Weight = {weight:.4f} (AUC={auc:.3f}, Prec={precision:.3f}, FNR={fn_rate:.3f})")
        
        # Normalize weights
        total_weight = sum(model_weights.values())
        model_weights = {k: v/total_weight for k, v in model_weights.items()}
        
        print(f"\nNormalized weights:")
        for model_name, weight in model_weights.items():
            print(f"  {model_name}: {weight:.3f}")
        
        # Create weighted ensemble prediction
        ensemble_pred_val = np.zeros(len(self.X_val))
        for i, (model_name, weight) in enumerate(model_weights.items()):
            ensemble_pred_val += weight * meta_features_val[:, i]
        
        # Optimize ensemble threshold
        best_threshold = 0.5
        best_score = -float('inf')
        
        thresholds = np.linspace(0.05, 0.95, 200)
        
        for threshold in thresholds:
            y_pred = (ensemble_pred_val >= threshold).astype(int)
            
            if len(np.unique(y_pred)) == 1:
                continue
            
            try:
                tn, fp, fn, tp = confusion_matrix(self.y_val, y_pred).ravel()
                
                precision = tp / (tp + fp) if (tp + fp) > 0 else 0
                recall = tp / (tp + fn) if (tp + fn) > 0 else 0
                fn_rate = fn / (fn + tp) if (fn + tp) > 0 else 0
                
                # Same multi-objective scoring
                score = (
                    0.6 * precision +
                    0.3 * (1 - fn_rate) +
                    0.1 * recall
                )
                
                if precision >= 0.85:
                    score += 0.2
                if precision >= 0.90:
                    score += 0.3
                
                if fn_rate > 0.4:
                    score -= 0.3
                if fn_rate > 0.6:
                    score -= 0.5
                
                if score > best_score:
                    best_score = score
                    best_threshold = threshold
            except:
                continue
        
        self.ensemble = {
            'model_names': model_names,
            'model_weights': model_weights,
            'threshold': best_threshold,
            'method': 'weighted_average'
        }
        
        print(f"\nWeighted ensemble created with optimized threshold: {best_threshold:.4f}")
        print(f"Best multi-objective score: {best_score:.4f}")
        
        return self.ensemble
    
    def comprehensive_evaluation(self):
        """Comprehensive evaluation focused on business metrics"""
        print(f"\n" + "="*80)
        print("STEP 8: COMPREHENSIVE BUSINESS-FOCUSED EVALUATION")
        print("="*80)
        
        evaluation_results = {}
        
        # Evaluate individual models
        print("\nIndividual Model Performance on Test Set:")
        print("-" * 70)
        
        for model_name, model in self.models.items():
            # Get test predictions
            if model_name == 'Optimized_LightGBM':
                test_proba = model.predict(self.X_test, num_iteration=model.best_iteration)
            elif model_name == 'Optimized_XGBoost':
                test_proba = model.predict(xgb.DMatrix(self.X_test))
            elif model_name == 'Optimized_LogisticRegression':
                test_proba = model['model'].predict_proba(model['scaler'].transform(self.X_test))[:, 1]
            else:
                test_proba = model.predict_proba(self.X_test)[:, 1]
            
            # Use optimized threshold
            threshold = self.optimal_thresholds[model_name]['threshold']
            test_pred = (test_proba >= threshold).astype(int)
            
            # Calculate metrics
            auc = roc_auc_score(self.y_test, test_proba)
            precision = precision_score(self.y_test, test_pred)
            recall = recall_score(self.y_test, test_pred)
            f1 = f1_score(self.y_test, test_pred)
            
            cm = confusion_matrix(self.y_test, test_pred)
            tn, fp, fn, tp = cm.ravel()
            
            fn_rate = fn / (fn + tp) if (fn + tp) > 0 else 0
            fp_rate = fp / (fp + tn) if (fp + tn) > 0 else 0
            
            # Business cost calculation
            total_cost = (fn * self.fn_penalty) + (fp * self.fp_penalty)
            
            evaluation_results[model_name] = {
                'auc': auc,
                'precision': precision,
                'recall': recall,
                'f1_score': f1,
                'threshold': threshold,
                'confusion_matrix': cm.tolist(),
                'false_negative_rate': fn_rate,
                'false_positive_rate': fp_rate,
                'total_cost': total_cost,
                'true_negatives': int(tn),
                'false_positives': int(fp),
                'false_negatives': int(fn),
                'true_positives': int(tp)
            }
            
            print(f"\n{model_name}:")
            print(f"  AUC: {auc:.4f}")
            print(f"  Precision: {precision:.4f}")
            print(f"  Recall: {recall:.4f}")
            print(f"  FN Rate: {fn_rate:.4f}")
            print(f"  Business Cost: {total_cost:.2f}")
            print(f"  Confusion: TN={tn}, FP={fp}, FN={fn}, TP={tp}")
        
        # Evaluate weighted ensemble
        print(f"\nWeighted Ensemble Performance:")
        print("-" * 40)
        
        # Get ensemble predictions on test set
        meta_features_test = np.zeros((len(self.X_test), len(self.models)))
        
        for i, (model_name, model) in enumerate(self.models.items()):
            if model_name == 'Optimized_LightGBM':
                pred = model.predict(self.X_test, num_iteration=model.best_iteration)
            elif model_name == 'Optimized_XGBoost':
                pred = model.predict(xgb.DMatrix(self.X_test))
            elif model_name == 'Optimized_LogisticRegression':
                pred = model['model'].predict_proba(model['scaler'].transform(self.X_test))[:, 1]
            else:
                pred = model.predict_proba(self.X_test)[:, 1]
            
            meta_features_test[:, i] = pred
        
        # Apply weights
        ensemble_proba = np.zeros(len(self.X_test))
        for i, (model_name, weight) in enumerate(self.ensemble['model_weights'].items()):
            ensemble_proba += weight * meta_features_test[:, i]
        
        ensemble_pred = (ensemble_proba >= self.ensemble['threshold']).astype(int)
        
        # Calculate ensemble metrics
        ensemble_auc = roc_auc_score(self.y_test, ensemble_proba)
        ensemble_precision = precision_score(self.y_test, ensemble_pred)
        ensemble_recall = recall_score(self.y_test, ensemble_pred)
        ensemble_f1 = f1_score(self.y_test, ensemble_pred)
        
        ensemble_cm = confusion_matrix(self.y_test, ensemble_pred)
        tn, fp, fn, tp = ensemble_cm.ravel()
        
        ensemble_fn_rate = fn / (fn + tp) if (fn + tp) > 0 else 0
        ensemble_fp_rate = fp / (fp + tn) if (fp + tn) > 0 else 0
        ensemble_total_cost = (fn * self.fn_penalty) + (fp * self.fp_penalty)
        
        evaluation_results['Weighted_Ensemble'] = {
            'auc': ensemble_auc,
            'precision': ensemble_precision,
            'recall': ensemble_recall,
            'f1_score': ensemble_f1,
            'threshold': self.ensemble['threshold'],
            'confusion_matrix': ensemble_cm.tolist(),
            'false_negative_rate': ensemble_fn_rate,
            'false_positive_rate': ensemble_fp_rate,
            'total_cost': ensemble_total_cost,
            'true_negatives': int(tn),
            'false_positives': int(fp),
            'false_negatives': int(fn),
            'true_positives': int(tp)
        }
        
        print(f"  AUC: {ensemble_auc:.4f}")
        print(f"  Precision: {ensemble_precision:.4f}")
        print(f"  Recall: {ensemble_recall:.4f}")
        print(f"  FN Rate: {ensemble_fn_rate:.4f}")
        print(f"  Business Cost: {ensemble_total_cost:.2f}")
        print(f"  Confusion: TN={tn}, FP={fp}, FN={fn}, TP={tp}")
        
        # Find best model by lowest cost
        best_model = min(evaluation_results.keys(), 
                        key=lambda x: evaluation_results[x]['total_cost'])
        
        print(f"\n" + "="*60)
        print(f"BEST MODEL (Lowest Business Cost): {best_model}")
        print("="*60)
        best_metrics = evaluation_results[best_model]
        print(f"üéØ AUC: {best_metrics['auc']:.4f} (Target: ‚â•0.85)")
        print(f"üéØ Precision: {best_metrics['precision']:.4f} (Target: ‚â•0.90)")
        print(f"üéØ False Negative Rate: {best_metrics['false_negative_rate']:.4f} (Target: ‚â§0.10)")
        print(f"üí∞ Business Cost: {best_metrics['total_cost']:.2f} (Minimized)")
        print(f"üìä False Negatives: {best_metrics['false_negatives']} (Critical to minimize)")
        
        # Check target achievement with better messaging
        print(f"\nüéØ TARGET ACHIEVEMENT ANALYSIS:")
        
        auc_achieved = best_metrics['auc'] >= 0.85
        precision_achieved = best_metrics['precision'] >= 0.90
        fn_rate_achieved = best_metrics['false_negative_rate'] <= 0.10
        
        auc_status = '‚úÖ ACHIEVED' if auc_achieved else f'‚ö†Ô∏è CLOSE ({best_metrics["auc"]:.1%})' if best_metrics['auc'] >= 0.80 else '‚ùå MISSED'
        precision_status = '‚úÖ ACHIEVED' if precision_achieved else f'‚ö†Ô∏è CLOSE ({best_metrics["precision"]:.1%})' if best_metrics['precision'] >= 0.85 else '‚ùå MISSED'
        fn_status = '‚úÖ ACHIEVED' if fn_rate_achieved else f'‚ö†Ô∏è CLOSE ({best_metrics["false_negative_rate"]:.1%})' if best_metrics['false_negative_rate'] <= 0.15 else '‚ùå MISSED'

        print(f"   AUC ‚â• 85%: {auc_status}")
        print(f"   Precision ‚â• 90%: {precision_status}")
        print(f"   FN Rate ‚â§ 10%: {fn_status}")
        
        # Calculate improvement from baseline
        print(f"\nüìà IMPROVEMENT ANALYSIS:")
        print(f"   False Negatives: {best_metrics['false_negatives']} (Target: <5, Previous: 39)")
        if best_metrics['false_negatives'] < 39:
            improvement = ((39 - best_metrics['false_negatives']) / 39) * 100
            print(f"   FN Improvement: {improvement:.1f}% reduction ‚úÖ")
        
        self.evaluation_results = evaluation_results
        return evaluation_results, best_model
    
    def create_visualizations(self):
        """Create improved visualizations"""
        print(f"\n" + "="*80)
        print("STEP 9: CREATING IMPROVED VISUALIZATIONS")
        print("="*80)
        
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        
        # 1. Performance Comparison
        models = list(self.evaluation_results.keys())
        metrics = ['auc', 'precision', 'recall']
        
        x = np.arange(len(models))
        width = 0.25
        
        for i, metric in enumerate(metrics):
            values = [self.evaluation_results[model][metric] for model in models]
            axes[0, 0].bar(x + i*width, values, width, label=metric.upper())
        
        axes[0, 0].axhline(y=0.85, color='red', linestyle='--', alpha=0.7, label='AUC Target')
        axes[0, 0].axhline(y=0.90, color='green', linestyle='--', alpha=0.7, label='Precision Target')
        axes[0, 0].set_xlabel('Models')
        axes[0, 0].set_ylabel('Score')
        axes[0, 0].set_title('IMPROVED Model Performance vs Targets')
        axes[0, 0].set_xticks(x + width)
        axes[0, 0].set_xticklabels(models, rotation=45, ha='right')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # 2. Business Cost Analysis
        costs = [self.evaluation_results[model]['total_cost'] for model in models]
        fn_counts = [self.evaluation_results[model]['false_negatives'] for model in models]
        
        # Dual axis plot
        ax2_twin = axes[0, 1].twinx()
        
        bars1 = axes[0, 1].bar(models, costs, alpha=0.7, color='blue', label='Business Cost')
        bars2 = ax2_twin.bar(models, fn_counts, alpha=0.7, color='red', label='False Negatives')
        
        axes[0, 1].set_ylabel('Business Cost', color='blue')
        ax2_twin.set_ylabel('False Negatives Count', color='red')
        axes[0, 1].set_title('Business Cost vs False Negatives')
        axes[0, 1].tick_params(axis='x', rotation=45)
        
        # Add target line for FN
        ax2_twin.axhline(y=5, color='green', linestyle='--', alpha=0.7, label='FN Target (‚â§5)')
        
        # 3. Precision-Recall with FN Rate
        precisions = [self.evaluation_results[model]['precision'] for model in models]
        recalls = [self.evaluation_results[model]['recall'] for model in models]
        fn_rates = [self.evaluation_results[model]['false_negative_rate'] for model in models]
        
        # Size bubbles by inverse of FN rate
        bubble_sizes = [(1 - fn_rate) * 200 for fn_rate in fn_rates]
        
        scatter = axes[0, 2].scatter(recalls, precisions, s=bubble_sizes, alpha=0.7, c=fn_rates, cmap='RdYlGn_r')
        
        for i, model in enumerate(models):
            axes[0, 2].annotate(model, (recalls[i], precisions[i]), 
                               xytext=(5, 5), textcoords='offset points', fontsize=8)
        
        axes[0, 2].axhline(y=0.9, color='r', linestyle='--', alpha=0.7, label='Target Precision (90%)')
        axes[0, 2].set_xlabel('Recall')
        axes[0, 2].set_ylabel('Precision')
        axes[0, 2].set_title('Precision vs Recall (Bubble Size = Low FN Rate)')
        axes[0, 2].legend()
        axes[0, 2].grid(True, alpha=0.3)
        
        # Add colorbar
        plt.colorbar(scatter, ax=axes[0, 2], label='FN Rate')
        
        # 4. Feature Importance
        if 'Optimized_RandomForest' in self.models:
            rf_model = self.models['Optimized_RandomForest']
            importances = rf_model.feature_importances_
            indices = np.argsort(importances)[::-1][:15]  # Top 15
            
            axes[1, 0].barh(range(len(indices)), importances[indices])
            axes[1, 0].set_yticks(range(len(indices)))
            axes[1, 0].set_yticklabels([self.feature_names[i] for i in indices])
            axes[1, 0].set_xlabel('Feature Importance')
            axes[1, 0].set_title('Top 15 Most Important Features')
            axes[1, 0].grid(True, alpha=0.3)
        
        # 5. Threshold Analysis for Best Model
        best_model = min(models, key=lambda x: self.evaluation_results[x]['total_cost'])
        if best_model == 'Weighted_Ensemble':
            threshold = self.ensemble['threshold']
            axes[1, 1].axvline(x=threshold, color='red', linestyle='--', 
                            label=f'Ensemble Threshold ({threshold:.3f})')
            axes[1, 1].set_title(f'Threshold Analysis - {best_model}')
        else:
            threshold = self.optimal_thresholds[best_model]['threshold']
            axes[1, 1].axvline(x=threshold, color='red', linestyle='--', 
                            label=f'Optimal Threshold ({threshold:.3f})')
            axes[1, 1].set_title(f'Threshold Analysis - {best_model}')

        axes[1, 1].set_xlabel('Threshold')
        axes[1, 1].set_ylabel('Metric Value')
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3)
        
        axes[1, 1].axvline(x=threshold, color='red', linestyle='--', 
                          label=f'Optimal Threshold ({threshold:.3f})')
        axes[1, 1].set_xlabel('Threshold')
        axes[1, 1].set_ylabel('Metric Value')
        axes[1, 1].set_title(f'Threshold Analysis - {best_model}')
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3)
        
        # 6. Confusion Matrix for Best Model
        cm = np.array(self.evaluation_results[best_model]['confusion_matrix'])
        
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1, 2])
        axes[1, 2].set_xlabel('Predicted')
        axes[1, 2].set_ylabel('Actual')
        axes[1, 2].set_title(f'Confusion Matrix - {best_model}')
        
        plt.tight_layout()
        plt.savefig('improved_analysis_results.png', dpi=150, bbox_inches='tight')
        print("‚úÖ IMPROVED analysis visualization saved to: improved_analysis_results.png")
        plt.show()
    
    def save_complete_results(self, output_dir='improved_results'):
        """Save all improved results"""
        print(f"\n" + "="*80)
        print("STEP 10: SAVING IMPROVED RESULTS")
        print("="*80)
        
        os.makedirs(output_dir, exist_ok=True)
        
        # Save models
        for model_name, model in self.models.items():
            model_path = os.path.join(output_dir, f'{model_name.lower()}.pkl')
            with open(model_path, 'wb') as f:
                pickle.dump({
                    'model': model,
                    'feature_names': self.feature_names,
                    'optimal_threshold': self.optimal_thresholds[model_name],
                    'overfitting_analysis': self.overfitting_results[model_name]
                }, f)
        
        # Save weighted ensemble
        ensemble_path = os.path.join(output_dir, 'weighted_ensemble.pkl')
        with open(ensemble_path, 'wb') as f:
            pickle.dump({
                'ensemble': self.ensemble,
                'base_models': self.models,
                'feature_names': self.feature_names
            }, f)
        
        # Convert numpy types
        def convert_numpy_types(obj):
            if isinstance(obj, np.bool_):
                return bool(obj)
            elif isinstance(obj, (np.integer, np.floating)):
                return obj.item()
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            return obj
        
        # Save comprehensive results
        results_summary = {}
        for k, v in self.evaluation_results.items():
            results_summary[k] = {}
            for key, val in v.items():
                results_summary[k][key] = convert_numpy_types(val)
        
        final_summary = {
            'evaluation_results': results_summary,
            'configuration': {
                'fn_penalty': self.fn_penalty,
                'fp_penalty': self.fp_penalty,
                'target_precision': self.target_precision,
                'improvements_applied': [
                    'Phase 1: Fixed overfitting + data distribution',
                    'Phase 2: Optimized for downturn detection performance',
                    'Sweet spot FN penalty (5x)',
                    'Enhanced 20-feature set',
                    'Precision-focused threshold optimization',
                    'Weighted ensemble based on downturn detection ability',
                    'Multi-objective scoring (precision + FN rate + recall)'
                ]
            },
            'feature_engineering': {
                'total_features': len(self.feature_names),
                'feature_names': self.feature_names
            },
            'best_model': min(self.evaluation_results.keys(), 
                            key=lambda x: self.evaluation_results[x]['total_cost']),
            'targets_achieved': {
                'precision_90': convert_numpy_types(max([self.evaluation_results[k]['precision'] for k in self.evaluation_results.keys()]) >= 0.90),
                'auc_85': convert_numpy_types(max([self.evaluation_results[k]['auc'] for k in self.evaluation_results.keys()]) >= 0.85),
                'fn_rate_10': convert_numpy_types(min([self.evaluation_results[k]['false_negative_rate'] for k in self.evaluation_results.keys()]) <= 0.10)
            },
            'training_date': datetime.now().isoformat()
        }
        
        with open(os.path.join(output_dir, 'improved_results_summary.json'), 'w') as f:
            json.dump(final_summary, f, indent=2)
        
        print(f"‚úÖ IMPROVED results saved to: {output_dir}/")
        print("Saved components:")
        print("  ‚úÖ Performance-optimized models")
        print("  ‚úÖ Weighted ensemble for downturn detection")
        print("  ‚úÖ Precision-focused threshold optimization")
        print("  ‚úÖ Enhanced feature set (20 features)")
        print("  ‚úÖ Business-focused evaluation metrics")
        print("  ‚úÖ Complete improvement documentation")
        
        return output_dir
    
    def run_complete_pipeline(self):
        """Run the complete IMPROVED pipeline"""
        print("STARTING IMPROVED ENHANCED ASI DOWNTURN PREDICTION PIPELINE")
        print("="*100)
        
        try:
            # Step 1: Enhanced Feature Engineering
            self.engineer_enhanced_features()
            
            # Step 2: Create Labels
            self.create_labels(threshold=-0.05, prediction_days=10)
            
            # Step 3: Enhanced Dataset Preparation
            self.prepare_and_save_datasets_improved(test_size=0.2, val_size=0.2)
            
            # Step 4: Performance-Optimized Model Training
            self.train_performance_optimized_models()
            
            # Step 5: Overfitting Check
            self.detect_overfitting()
            
            # Step 6: Precision-Focused Threshold Optimization
            self.optimize_precision_focused_thresholds()
            
            # Step 7: Weighted Ensemble
            self.create_weighted_ensemble()
            
            # Step 8: Business-Focused Evaluation
            evaluation_results, best_model = self.comprehensive_evaluation()
            
            # Step 9: Improved Visualizations
            self.create_visualizations()
            
            # Step 10: Save Results
            output_dir = self.save_complete_results()
            
            print(f"\n" + "="*100)
            print("üéâ IMPROVED ENHANCED PIPELINE COMPLETED! üéâ")
            print("="*100)
            
            best_metrics = evaluation_results[best_model]
            
            print(f"\nüìä FINAL IMPROVED RESULTS:")
            print(f"üèÜ Best Model: {best_model}")
            print(f"üéØ AUC: {best_metrics['auc']:.4f} (Target: ‚â•0.85)")
            print(f"üéØ Precision: {best_metrics['precision']:.4f} (Target: ‚â•0.90)")
            print(f"üéØ False Negative Rate: {best_metrics['false_negative_rate']:.4f} (Target: ‚â§0.10)")
            print(f"üìâ False Negatives: {best_metrics['false_negatives']} (Previous: 39)")
            print(f"üí∞ Business Cost: {best_metrics['total_cost']:.2f}")
            
            # Target achievement
            auc_achieved = best_metrics['auc'] >= 0.85
            precision_achieved = best_metrics['precision'] >= 0.90
            fn_rate_achieved = best_metrics['false_negative_rate'] <= 0.10
            
            print(f"\nüèÅ FINAL TARGET ACHIEVEMENT:")
            print(f"   AUC ‚â• 85%: {'‚úÖ ACHIEVED' if auc_achieved else '‚ö†Ô∏è CLOSE' if best_metrics['auc'] >= 0.82 else '‚ùå MISSED'}")
            print(f"   Precision ‚â• 90%: {'‚úÖ ACHIEVED' if precision_achieved else '‚ö†Ô∏è CLOSE' if best_metrics['precision'] >= 0.85 else '‚ùå MISSED'}")
            print(f"   FN Rate ‚â§ 10%: {'‚úÖ ACHIEVED' if fn_rate_achieved else '‚ö†Ô∏è CLOSE' if best_metrics['false_negative_rate'] <= 0.20 else '‚ùå MISSED'}")
            
            # Calculate overall success
            targets_achieved = sum([auc_achieved, precision_achieved, fn_rate_achieved])
            close_targets = sum([
                best_metrics['auc'] >= 0.82,
                best_metrics['precision'] >= 0.85,
                best_metrics['false_negative_rate'] <= 0.20
            ])
            
            if targets_achieved >= 2:
                print(f"\nüéØ üéâ MAJOR SUCCESS: {targets_achieved}/3 targets achieved! üéâ üéØ")
            elif close_targets >= 3:
                print(f"\n‚úÖ SIGNIFICANT IMPROVEMENT: All targets very close!")
            else:
                print(f"\nüìà GOOD PROGRESS: Further optimization possible")
            
            # Improvement summary
            fn_improvement = ((39 - best_metrics['false_negatives']) / 39) * 100 if best_metrics['false_negatives'] < 39 else 0
            
            print(f"\nüîß IMPROVEMENT SUMMARY:")
            print(f"   Overfitting: FIXED ‚úÖ")
            print(f"   Data Balance: FIXED ‚úÖ") 
            print(f"   False Negatives: {fn_improvement:.1f}% reduction ‚úÖ")
            print(f"   Feature Set: Enhanced to 20 key features ‚úÖ")
            print(f"   Ensemble: Weighted for downturn detection ‚úÖ")
            
            print(f"\nüìÅ All results saved to: {output_dir}/")
            print(f"üìà Visualization saved to: improved_analysis_results.png")
            
            return best_model, best_metrics, (targets_achieved >= 2)
            
        except Exception as e:
            print(f"\n‚ùå ERROR in pipeline: {str(e)}")
            import traceback
            traceback.print_exc()
            return None, None, False

def main():
    """Main function to run the IMPROVED enhanced predictor"""
    csv_file = 'ASI_close_prices_last_15_years_2025-12-05.csv'
    
    # Check if file exists
    if not os.path.exists(csv_file):
        print(f"‚ùå Dataset file not found: {csv_file}")
        print("Please ensure the CSV file is in the current directory.")
        return None
    
    # Initialize IMPROVED enhanced predictor
    predictor = ImprovedEnhancedPredictor(
        csv_file=csv_file,
        fn_penalty=5.0,         # SWEET SPOT between 2x and 20x
        target_precision=0.92
    )
    
    # Run IMPROVED pipeline
    best_model, results, major_success = predictor.run_complete_pipeline()
    
    return predictor, best_model, results, major_success

if __name__ == "__main__":
    predictor, best_model, results, major_success = main()